<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Compose visual entities and relationships in LLMs via communicative decoding.">
  <meta name="keywords" content="Vision-language Models, Compositionality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Compositional VLM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- Navigation bar. -->
<nav class="navbar is-fixed-top is-transparent">
  <div class="navbar-brand">
    <a class="navbar-item" href="https://compositionalvlm.github.io">
      <span style="font-weight:bolder; font-size: 22px; color: rgb(191, 91, 108);">Compositional VLM</span>
    </a>
  </div>
  <div id="navbarMenuHeroA" class="navbar-menu">
    <div class="navbar-start">
      <a class="navbar-item" href="#abstract">
        Abstract
      </a>
      <a class="navbar-item" href="#method">
        Method
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Showcases
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="#hoi">
            Human-object Interaction Detection
          </a>
          <a class="navbar-item" href="#caption">
            Image Captioning
          </a>
          <a class="navbar-item" href="#vqa">
            Visual Question Answering
          </a>
          <a class="navbar-item" href="#refcoco">
            Referring Expression Comprehension
          </a>
          <a class="navbar-item" href="#aro">
            Compositional Entity Prediction
          </a>
          <a class="navbar-item" href="#cola">
            Compositional Text-to-image Retrieval
          </a>
        </div>
      </div>
      <a class="navbar-item" href="#dataset">
        Dataset
      </a>
      <a class="navbar-item" href="#results">
        Results
      </a>
    </div>
    <div class="navbar-end">
      <div class="navbar-item">
        <div class="buttons">
          <a class="button is-primary is-outlined" href="https://huggingface.co/compositionalVLM/CompositionalVLM">
            <strong>Huggingface Demo</strong>
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h1>
          <h3 class="title is-5 conference-authors">ICLR 2024 Submission</h3>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their "bag-of-words" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose Compositional VLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<div class="container is-max-desktop" id="method">
  <hr class="solid">
</div>

<section class="hero">
  <div class="container is-fullhd">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-3">Method</h2>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div class="content">
          <p>
            <strong style="font-weight: 900;">Compositional VLM</strong> features a novel communicative decoding mechanism: communicative tokens bridge the vision system and the language system, and the vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated.
          </p>
        </div>
        <div class="columns is-vcentered is-centered">
          <video id="teaser" autoplay muted loop width="80%">
        <source src="./static/videos/method.mp4"
                type="video/mp4">
          </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<div class="container is-max-desktop">
  <hr class="solid">
  <div class="column is-full-width is-centered has-text-centered">
    <h2 class="title is-3">Showcases</h2>
  </div>
</div>



<section class="hero" id="hoi">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Human-object Interaction (HOI) Detection</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="hoi" autoplay muted loop  width="30%">
        <source src="./static/videos/hoi1_ready.mp4"
                type="video/mp4">
          </video>

          <video id="hoi1" autoplay muted loop width="30%">
            <source src="./static/videos/hoi2_ready.mp4"
                    type="video/mp4">
          </video>

          <video id="hoi2" autoplay muted loop width="30%">
            <source src="./static/videos/hoi3_ready.mp4"
                    type="video/mp4">
          </video>

          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="hero" id="caption">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Image Captioning</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="caption1" autoplay muted loop  width="50%">
        <source src="./static/videos/cap1_ready.mp4"
                type="video/mp4">
          </video>

          <!-- <video id="caption2" autoplay muted loop width="45%">
            <source src="./static/videos/cap1_ready.mp4"
                    type="video/mp4">
          </video> -->
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="hero" id="vqa">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Visual Question Answering</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="vqa1" autoplay muted loop  width="30%">
        <source src="./static/videos/vqa1_ready.mp4"
                type="video/mp4">
          </video>
          <video id="vqa2" autoplay muted loop  width="30%">
            <source src="./static/videos/vqa2_ready.mp4"
                    type="video/mp4">
          </video>
          <video id="vqa3" autoplay muted loop  width="30%">
            <source src="./static/videos/vqa3_ready.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="hero" id="refcoco">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Referring Expression Comprehension</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="ref1" autoplay muted loop  width="40%">
        <source src="./static/videos/refcoco1_ready.mp4"
                type="video/mp4">
          </video>
          <video id="ref2" autoplay muted loop  width="40%">
            <source src="./static/videos/refcoco2_ready.mp4"
                    type="video/mp4">
              </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero" id="aro">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Compositional Entity Prediction</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <p>Coming soon...</p>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section> -->



<!-- <section class="hero" id="cola">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Compositional Text-to-image Retrieval</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <p>Coming soon...</p>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section> -->


<div class="container is-max-desktop" id="dataset">
  <hr class="solid">
</div>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <p>
            For multimodal pre-training, we create a large-scale grounded image-text dataset which consists of over 97M
            image-text pairs from the pre-training data of BLIP-2. We apply a grounding pipeline to the image-text pair
            to associate the text spans in the caption to their corresponding visual entities in the image. The
            pipeline consists of three steps:
          </p>
        </div>
      </div>
    </div>

    <div class="card">
      <header class="card-header">
        <p class="card-header-title">
          Step 1: Generating bounding-box-word pairs
        </p>
      </header>
      <div class="card-content">
        <div class="content">
          We use GroundingDINO to detect objects in the image and link the bounding box of the object to words in the text.
        </div>
      </div>
    </div>


    <div class="card">
      <header class="card-header">
        <p class="card-header-title">
          Step 2: Expanding grounded words to grounded expressions
        </p>
      </header>
      <div class="card-content">
        <div class="content">
          Inspired by KOSMOS-2, we apply spaCy to obtain each word's dependency relation in the sentence, and expand a grounded word to a grounded expression by recursively traversing the dependency tree of that word and concatenate eligible children words based on the linguistic rules.
        </div>
      </div>
    </div>


    <div class="card">
      <header class="card-header">
        <p class="card-header-title">
          Step 3: Assigning bounding boxes to the special communication tokens
        </p>
      </header>
      <div class="card-content">
        <div class="content">
          Given the expressions and their associated bounding boxes in a grounded image-text pair, we can now insert the special communication tokens into the text and assign the bounding boxes to them
        </div>
      </div>
    </div>
  </div>
</section>


<div class="container is-max-desktop" id="results">
  <hr class="solid">
  <div class="column is-full-width is-centered has-text-centered">
    <h2 class="title is-3">Results</h2>
    <p>click to expand</p>
  </div>
</div>


<section class="section">
  <div class="container is-max-desktop" style="margin-top: 20px;">
    <div class="card">
      <header class="card-header">
        <p class="card-header-title card-toggle">
          Compositional Entity Prediction
        </p>
        <a class="card-header-icon card-toggle">
          <i class="fa fa-angle-down"></i>
        </a>
      </header>
      <div class="card-content is-hidden">
        <canvas id="aro_chart" style="width:100%;"></canvas>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 20px;">
    <div class="card">
      <header class="card-header">
        <p class="card-header-title card-toggle">
          Compositional Text-to-image Retrieval
        </p>
        <a class="card-header-icon card-toggle">
          <i class="fa fa-angle-down"></i>
        </a>
      </header>
      <div class="card-content is-hidden">
        <canvas id="cola_chart" style="width:100%;"></canvas>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 20px;">
    <div class="card">
      <header class="card-header">
        <p class="card-header-title card-toggle">
          Human-Object Interaction Detection
        </p>
        <a class="card-header-icon card-toggle">
          <i class="fa fa-angle-down"></i>
        </a>
      </header>
      <div class="card-content is-hidden">
        <canvas id="hico_chart" style="width:100%;"></canvas>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 20px;">
    <div class="card">
      <header class="card-header">
        <p class="card-header-title card-toggle">
          Referring Expression Comprehension
        </p>
        <a class="card-header-icon card-toggle">
          <i class="fa fa-angle-down"></i>
        </a>
      </header>
      <div class="card-content is-hidden">
        <canvas id="refcoco_chart" style="width:100%;"></canvas>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 20px;">
    <div class="card">
      <header class="card-header">
        <p class="card-header-title card-toggle">
          Visual Question Answering
        </p>
        <a class="card-header-icon card-toggle">
          <i class="fa fa-angle-down"></i>
        </a>
      </header>
      <div class="card-content is-hidden">
        <canvas id="vqa_chart" style="width:100%;"></canvas>
      </div>
    </div>
  </div>
</section>


<script
src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.4/Chart.js">
</script>

<script>
  const xValues = ["MetaLM", "VLKD", "OpenFlamingo3B", "Flamingo3B", "BLIP-2 ViT-L OPT-2.7B", "KOSMOS-2", "Ours 1.4B", "Ours 2.8B"];
  const yValues = [41.1, 44.5, 44.6, 49.2, 49.7, 45.6, 45.2, 48.8];
  const barColors = ["red", "green", "yellow", "orange", "brown", "pink", "blue", "purple"];
  
  new Chart("vqa_chart", {
    type: "bar",
    data: {
      labels: xValues,
      datasets: [{
        backgroundColor: barColors,
        data: yValues
      }]
    },
    options: {
      legend: {display: false},
      title: {
        display: true,
        text: "VQAv2 Test-dev Zero-shot Accuracy (%)"
      }
    }
  });
</script>


<script>
  const refcoco_xValues =  ["ReCLIP", "KOSMOS-2", "Ours 1.4B", "Ours 2.8B"];
  const refcocog_yValues = [59.33, 60.57, 60.87, 61.23];
  const refcocop_yValues = [47.87, 45.48, 47.62, 48.87];
  const refcoco_yValues =  [45.78, 52.32, 48.19, 49.32];

  new Chart("refcoco_chart", {
    type: "bar",
    data: {
      labels: refcoco_xValues,
      datasets: [
          {
            label: "RefCOCOg",
            backgroundColor: "blue",
            data: refcocog_yValues
          },
          {
            label: "RefCOCO+",
            backgroundColor: "red",
            data: refcocop_yValues
          },
          {
            label: "RefCOCO",
            backgroundColor: "purple",
            data: refcoco_yValues
          },
      ]
    },
    options: {
      legend: {display: false},
      title: {
        display: true,
        text: "Referring Expression Comprehension Zero-shot Accuracy (%)"
      }
    }
  });
</script>



<script>
  const hico_xValues =  ["RLIPv1-ParSe", "RLIPv2-ParSeDA", "KOSMOS-2", "Ours 1.4B"];
  const rare_yValues =    [15.08, 27.97, 33.51, 50.82];
  const nonrare_yValues = [15.50, 21.90, 17.83, 35.47];
  const full_yValues =    [15.40, 23.29, 21.26, 39.00];

  new Chart("hico_chart", {
    type: "bar",
    data: {
      labels: hico_xValues,
      datasets: [
          {
            label: "Rare",
            backgroundColor: "blue",
            data: rare_yValues
          },
          {
            label: "Non-Rare",
            backgroundColor: "red",
            data: nonrare_yValues
          },
          {
            label: "Full",
            backgroundColor: "purple",
            data: full_yValues
          },
      ]
    },
    options: {
      legend: {display: false},
      title: {
        display: true,
        text: "HICO-DET Zero-shot mAP (%)"
      }
    }
  });
</script>




<script>
  const cola_xValues = ["CLIP", "FLAVA", "OpenFlamingo3B", "BLIP", "BLIP-2 ViT-L OPT-2.7B", "KOSMOS-2", "Ours 1.4B"];
  const cola_yValues = [21.42, 24.76, 18.10, 41.43, 35.71, 30.48, 44.29];
  const cola_barColors = ["red", "green", "yellow", "orange", "brown", "pink", "blue"];
  
  new Chart("cola_chart", {
    type: "bar",
    data: {
      labels: cola_xValues,
      datasets: [{
        backgroundColor: cola_barColors,
        data: cola_yValues
      }]
    },
    options: {
      legend: {display: false},
      title: {
        display: true,
        text: "Cola Zero-shot Accuracy (%)"
      }
    }
  });
</script>



<script>
  const aro_xValues =  ["CLIP", "FLAVA", "OpenFlamingo3B", "BLIP", "BLIP-2 ViT-L OPT-2.7B", "KOSMOS-2", "Ours 1.4B"];
  const aro1_yValues = [6.93, 4.59, 2.55, 29.78, 29.73, 19.88, 32.46];
  const aro5_yValues = [21.12, 12.76, 7.11, 54.18, 54.91, 43.69, 55.70];

  new Chart("aro_chart", {
    type: "bar",
    data: {
      labels: aro_xValues,
      datasets: [
          {
            label: "Top-1 Accuracy",
            backgroundColor: "blue",
            data: aro1_yValues
          },
          {
            label: "Top-5 Accuracy",
            backgroundColor: "red",
            data: aro5_yValues
          },
      ]
    },
    options: {
      legend: {display: false},
      title: {
        display: true,
        text: "ARO Entity Prediction Zero-shot Accuracy (%)"
      }
    }
  });
</script>


<script>
  document.addEventListener('DOMContentLoaded', function() {
    let cardToggles = document.getElementsByClassName('card-toggle');
    for (let i = 0; i < cardToggles.length; i++) {
      cardToggles[i].addEventListener('click', e => {
        e.currentTarget.parentElement.parentElement.childNodes[3].classList.toggle('is-hidden');
      });
    }
  });
</script>


<script>
  // const collapsibles = document.querySelectorAll('.collapsible');
  // collapsibles.forEach(collapsible => {
  //   const content = collapsible.nextElementSibling;
  //   collapsible.addEventListener('click', () => {
  //     content.classList.toggle('active'); 
  //     if (content.style.display === "block") {
  //       content.style.display = "none";
  //     } else {
  //       content.style.display = "block";
  //     }     
  //     if (content.classList.contains('active')) {
  //       videos.forEach(video => {
  //         video.src = video.getAttribute('data-src');
  //       });
  //     }
  //   });
  // });
  // Get the button and content elements
  const collapsibles = document.querySelectorAll('.collapsible');
    collapsibles.forEach(collapsible => {
    const content = collapsible.nextElementSibling;

    // Add an event listener to the button to toggle the content
    collapsible.addEventListener('click', function() {
      this.classList.toggle("active");
      if (content.style.display === 'block') {
        content.style.display = 'none';
        this.innerHTML = "Click to show videos";
      } else {
        content.style.display = 'block';
        this.innerHTML = "Click to hide videos";
        
        // Find all the video elements within the content
        const videos = content.querySelectorAll('video');
        
        // Set the src attribute of each video element
        videos.forEach(video => {
          if (!video.loaded) {
            const videoSrc = video.getAttribute('data-src');
            if (videoSrc) {
              video.src = videoSrc;
              video.loaded = true;
            }
          }
        });
      }
    });
  });
// var buttons = document.querySelectorAll('.collapsible');

// // Loop through the buttons and add a click event listener to each one
// for (var i = 0; i < buttons.length; i++) {
//   buttons[i].addEventListener('click', function() {

//     // Get the content element that corresponds to this button
//     var content = this.nextElementSibling;
//     const videos = content.querySelectorAll('video');


//     buttons[i].addEventListener('click', () => {
//       content.classList.toggle('active');
//     });
//     if (content.style.display === "block") {
//       content.style.display = "none";
//     } else {
//       content.style.display = "block";
//     }

//   });
// }

</script>


<!-- Default Statcounter code for compositionalvlm
https://compositionalvlm.github.io/ -->
<script type="text/javascript">
  var sc_project=12930260; 
  var sc_invisible=1; 
  var sc_security="f93f9c3b"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12930260/0/f93f9c3b/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

</body>
</html>
