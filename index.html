<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Compose visual entities and relationships in LLMs via communicative decoding.">
  <meta name="keywords" content="Vision-language Models, Compositionality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Compositional VLM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- Navigation bar. -->
<nav class="navbar is-fixed-top is-transparent">
  <div class="navbar-brand">
    <a class="navbar-item" href="https://compositionalvlm.github.io">
      <span style="font-weight:bolder; font-size: 22px; color: rgb(191, 91, 108);">Compositional VLM</span>
    </a>
  </div>
  <div id="navbarMenuHeroA" class="navbar-menu">
    <div class="navbar-start">
      <a class="navbar-item" href="https://compositionalvlm.github.io/#abstract">
        Abstract
      </a>
      <a class="navbar-item" href="https://compositionalvlm.github.io/#method">
        Method
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Showcases
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item">
            Human-object Interaction Detection
          </a>
          <a class="navbar-item">
            Image Captioning
          </a>
          <a class="navbar-item">
            Visual Question Answering
          </a>
          <div class="navbar-item">
            Referring Expression Comprehension
          </div>
        </div>
      </div>
    </div>
    <div class="navbar-end">
      <div class="navbar-item">
        <div class="buttons">
          <a class="button is-primary is-outlined" href="https://huggingface.co/compositionalVLM/CompositionalVLM">
            <strong>Huggingface Demo</strong>
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h1>
          <h3 class="title is-5 conference-authors">ICLR 2024 Submission</h3>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their "bag-of-words" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose Compositional VLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<div class="container is-fullhd" id="method">
  <hr class="solid">
  <div class="column is-full-width is-centered has-text-centered">
    <h2 class="title is-3">Method</h2>
  </div>
</div>


<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div class="content">
          <p>
            <strong style="font-weight: 900;">Compositional VLM</strong> features a novel communicative decoding mechanism: communicative tokens bridge the vision system and the language system, and the vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated.
          </p>
        </div>
        <div class="columns is-vcentered is-centered">
          <video id="teaser" autoplay muted loop width="80%">
        <source src="./static/videos/method.mp4"
                type="video/mp4">
          </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<div class="container is-max-desktop">
  <hr class="solid">
  <div class="column is-full-width is-centered has-text-centered">
    <h2 class="title is-3">Showcases</h2>
  </div>
</div>



<section class="hero">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Human-object Interaction (HOI) Detection</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="hoi" autoplay muted loop  width="30%">
        <source src="./static/videos/hoi1_ready.mp4"
                type="video/mp4">
          </video>

          <video id="hoi1" autoplay muted loop width="30%">
            <source src="./static/videos/hoi2_ready.mp4"
                    type="video/mp4">
          </video>

          <video id="hoi2" autoplay muted loop width="30%">
            <source src="./static/videos/hoi3_ready.mp4"
                    type="video/mp4">
          </video>

          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Image Captioning</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="caption1" autoplay muted loop  width="75%">
        <source src="./static/videos/cap1_ready.mp4"
                type="video/mp4">
          </video>

          <!-- <video id="caption2" autoplay muted loop width="45%">
            <source src="./static/videos/cap1_ready.mp4"
                    type="video/mp4">
          </video> -->
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Visual Question Answering</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="vqa1" autoplay muted loop  width="75%">
        <source src="./static/videos/vqa1_ready.mp4"
                type="video/mp4">
          </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="container is-max-desktop">
    <div class="column is-full-width is-centered has-text-centered">
      <h2 class="title is-4">Referring Expression Comprehension</h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="vqa1" autoplay muted loop  width="75%">
        <source src="./static/videos/refcoco1_ready.mp4"
                type="video/mp4">
          </video>
          </br>
        </div>
        <br>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="100%">
        <source src="./static/videos/tasks.m4v"
                type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        Human
        </h2>
      </div>
    </div>
  </div>
</section> -->

<script>
  // const collapsibles = document.querySelectorAll('.collapsible');
  // collapsibles.forEach(collapsible => {
  //   const content = collapsible.nextElementSibling;
  //   collapsible.addEventListener('click', () => {
  //     content.classList.toggle('active'); 
  //     if (content.style.display === "block") {
  //       content.style.display = "none";
  //     } else {
  //       content.style.display = "block";
  //     }     
  //     if (content.classList.contains('active')) {
  //       videos.forEach(video => {
  //         video.src = video.getAttribute('data-src');
  //       });
  //     }
  //   });
  // });
  // Get the button and content elements
  const collapsibles = document.querySelectorAll('.collapsible');
    collapsibles.forEach(collapsible => {
    const content = collapsible.nextElementSibling;

    // Add an event listener to the button to toggle the content
    collapsible.addEventListener('click', function() {
      this.classList.toggle("active");
      if (content.style.display === 'block') {
        content.style.display = 'none';
        this.innerHTML = "Click to show videos";
      } else {
        content.style.display = 'block';
        this.innerHTML = "Click to hide videos";
        
        // Find all the video elements within the content
        const videos = content.querySelectorAll('video');
        
        // Set the src attribute of each video element
        videos.forEach(video => {
          if (!video.loaded) {
            const videoSrc = video.getAttribute('data-src');
            if (videoSrc) {
              video.src = videoSrc;
              video.loaded = true;
            }
          }
        });
      }
    });
  });
// var buttons = document.querySelectorAll('.collapsible');

// // Loop through the buttons and add a click event listener to each one
// for (var i = 0; i < buttons.length; i++) {
//   buttons[i].addEventListener('click', function() {

//     // Get the content element that corresponds to this button
//     var content = this.nextElementSibling;
//     const videos = content.querySelectorAll('video');


//     buttons[i].addEventListener('click', () => {
//       content.classList.toggle('active');
//     });
//     if (content.style.display === "block") {
//       content.style.display = "none";
//     } else {
//       content.style.display = "block";
//     }

//   });
// }

</script>


<!-- Default Statcounter code for compositionalvlm
https://compositionalvlm.github.io/ -->
<script type="text/javascript">
  var sc_project=12930260; 
  var sc_invisible=1; 
  var sc_security="f93f9c3b"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12930260/0/f93f9c3b/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

</body>
</html>
